{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b110f7f",
   "metadata": {},
   "source": [
    "# Cartpole Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c2bcb",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d65ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2bb919",
   "metadata": {},
   "source": [
    "### Create and reset the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\") #, render_mode=\"human\")\n",
    "test_env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);\n",
    "\n",
    "# Get the state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c14e97",
   "metadata": {},
   "source": [
    "### Create the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy function (neural network)\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, use_activation = True):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        # Policy activation and dropout\n",
    "        self.use_activation = use_activation\n",
    "        \n",
    "        # Define paramaeters for layers\n",
    "        self.num_hidden_layers = 1\n",
    "        self.dimensions = [state_size, 128, action_size]\n",
    "        \n",
    "        # Define layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(self.dimensions[i], self.dimensions[i+1]))\n",
    "            # Activation layer\n",
    "            if self.use_activation:\n",
    "                self.layers.append(nn.ReLU())\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(self.dimensions[-2], self.dimensions[-1]))\n",
    "        \n",
    "    # Forward pass of the policy\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Softmax to get probs\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Option to initialize under the glorot distribution\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629a70c",
   "metadata": {},
   "source": [
    "### Make the training graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306c726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_episodes = 300\n",
    "\n",
    "idxs = range(max_episodes)\n",
    "fig, ax = plt.subplots(1, figsize=(10,6))\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629e94f",
   "metadata": {},
   "source": [
    "### Import the necessary functions and train the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f4452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from reinforce import *\n",
    "import tqdm\n",
    "\n",
    "# Configure parameters\n",
    "n_runs = 1\n",
    "print_every= 20\n",
    "window_size = 25\n",
    "train_batch_size = 5\n",
    "test_batch_size = 30\n",
    "max_eps_length = 500\n",
    "normalize = True\n",
    "reward_threshold = 400\n",
    "discount_factor = 0.99\n",
    "\n",
    "# Initialize reward storage points\n",
    "train_rewards = torch.zeros(n_runs, max_episodes)\n",
    "test_rewards = torch.zeros(n_runs, max_episodes)\n",
    "losses = torch.zeros(n_runs, max_episodes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train policy using Reinforce with 0 baseline\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    policy = Policy()\n",
    "    policy = policy.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    \n",
    "    reinforce = Reinforce(normalize, max_eps_length, discount_factor, train_batch_size,\n",
    "                          test_batch_size, policy, optimizer)\n",
    "    \n",
    "    for episode in tqdm.tqdm(range(max_episodes), desc=f'Run: {run}'):\n",
    "        \n",
    "        loss, train_reward = reinforce.train(env, device)\n",
    "        \n",
    "        test_reward = reinforce.evaluate(test_env, device)\n",
    "        \n",
    "        train_rewards[run][episode] = train_reward\n",
    "        test_rewards[run][episode] = test_reward\n",
    "        losses[run][episode] = loss\n",
    "        \n",
    "        if episode % print_every == 0:\n",
    "            mean_train_rewards = torch.mean(train_rewards[run, max(0,episode-window_size):episode+1])\n",
    "            mean_test_rewards = torch.mean(test_rewards[run, max(0,episode-window_size):episode+1])\n",
    "            mean_losses = torch.mean(losses[run, max(0,episode-window_size):episode+1])\n",
    "        \n",
    "            print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:5.1f} ',\n",
    "                  f'| Mean Test Rewards: {mean_test_rewards:5.1f} | Mean Losses: {mean_losses:5.1f} |')\n",
    "        \n",
    "        if (mean_test_rewards>reward_threshold):\n",
    "            \n",
    "            print(f'Reached reward threshold in {episode} episodes')\n",
    "            torch.save(policy, \"VPG_128_1e3_unnorm.pth\") # Save best model\n",
    "            break\n",
    "            \n",
    "        ax.clear()\n",
    "        ax.plot(idxs, test_rewards[:run+1, :].mean(0))\n",
    "        ax.fill_between(idxs, test_rewards.min(0).values, test_rewards.max(0).values, alpha=0.1)\n",
    "        \n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80dcf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"VPG_32times3_1e3_norm_thresh400.png\") # Save training figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy, \"VPG_32times3_1e3_norm_thresh400.pth\") # Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77dc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
